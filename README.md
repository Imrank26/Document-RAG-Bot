This README is designed to be professional and technical, making it perfect for your GitHub profile. It reflects the architecture and workflow found in your project files.

---

# Document-Based RAG Chatbot

An end-to-end **Retrieval-Augmented Generation (RAG)** pipeline designed to provide context-aware answers from local PDF datasets. This system leverages **Mistral-7B** and **FAISS** to ensure accurate document retrieval and response generation.

## Key Features

**Automated Data Ingestion**: Loads and processes PDF files using `DirectoryLoader` and `PyPDFLoader`.


**Semantic Chunking**: Implements `RecursiveCharacterTextSplitter` to create manageable text segments while preserving context.


**Vector Storage**: Utilizes **FAISS** for efficient similarity search and local embedding storage.


**Mistral-7B Integration**: Connects with **Hugging Face** endpoints to use Mistral-7B for inference.


**Streamlit UI**: Provides a clean, interactive chat interface for real-time user engagement.



## Technical Stack

 
**Language**: Python 

 
**Framework**: LangChain 

 
**LLM**: Mistral-7B-Instruct-v0.3 

 
**Embeddings**: sentence-transformers/all-MiniLM-L6-v2 

 
**Vector Database**: FAISS (Facebook AI Similarity Search) 


**Interface**: Streamlit 



## Project Workflow

1. 
**Phase 1 (Memory Setup)**: Raw PDFs are loaded, split into 500-character chunks, and converted into vector embeddings.


2. 
**Phase 2 (LLM Chain)**: A `RetrievalQA` chain is established, connecting the FAISS index with the Mistral model using custom prompt templates.


3. 
**Phase 3 (UI Deployment)**: The application is served via Streamlit, utilizing `@st.cache_resource` for optimized vector store loading.



## Getting Started

### Prerequisites

* Python 3.9+
* Pipenv

### Installation

1. Clone the repository and install dependencies:
```bash
pipenv install langchain langchain_community langchain_huggingface faiss-cpu pypdf streamlit

```


2. Set up your environment variables:
```bash
export HF_TOKEN="your_huggingface_token"

```


3. Build the vector database:
```bash
python create_memory_for_llm.py

```

4. Run the application:
```bash
streamlit run medibot.py

```
---

## ARCHITECTURE

![RAG Architecture Diagram](assets/RAGArchitecture.jpeg)
This diagram illustrates a three-phase architecture for a **Retrieval-Augmented Generation (RAG)** chatbot system. It outlines how raw documents are processed into a searchable format and then integrated with a Large Language Model (LLM) to provide answers.

### **Phase 1: Knowledge Base Creation**

This phase focuses on converting unstructured data into a structured vector database.

* 
**Data Loading:** Raw PDF files are gathered and text is extracted as a knowledge source.


* 
**Chunking:** The extracted text is broken down into smaller "Chunks of Text" to ensure the data is manageable for the model.


* 
**Embedding Generation:** Each text chunk is passed through a **Hugging Face** model to create numerical "embeddings".


* 
**Vector Storage:** These embeddings are stored in a **FAISS** (Facebook AI Similarity Search) vector store, which acts as the system's knowledge base.



### **Phase 2: Retrieval and LLM Processing**

This phase handles the logic of finding the right information when a user asks a question.

* 
**Question Embedding:** When a user asks a question, it is converted into an embedding using the same model as the text chunks.


* 
**Semantic Search:** The system performs a search in the **FAISS** database to find text chunks that are mathematically similar to the user's question.


* 
**Ranking:** The search results are ranked to identify the most relevant pieces of information.


* 
**LLM Integration:** Using the **LangChain** framework, the top-ranked results and the original question are sent to the **LLM** (Large Language Model) to generate a response.



### **Phase 3: User Interface**

This is the final interaction layer where the user sees the output.

* 
**Frontend:** The interface is built using **Streamlit**, which displays the user's query and the final "Answer" generated by the RAG pipeline.






